{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Load Instruction Data from URL (with Local Caching)\n",
        "\n",
        "This code defines a utility function `download_and_load_file` that downloads a JSON dataset from a given URL if it doesn't already exist locally, and loads it for use.\n",
        "\n",
        "- Uses Python’s `urllib` with a disabled SSL verification context (⚠️ not secure for production).\n",
        "- If the file is not present, it downloads and writes the content to disk.\n",
        "- If the file already exists, it reads it directly.\n",
        "- Parses the JSON content and returns it.\n",
        "- Finally, it prints the number of instruction entries loaded from the dataset.\n",
        "\n",
        "The dataset used here is `instruction-data.json` hosted on GitHub, which contains instruction-response pairs for training or evaluating instruction-tuned language models.\n"
      ],
      "metadata": {
        "id": "TNcIAtQhxLfw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG_ddkiWkZbc",
        "outputId": "5d79d971-8f1b-49c1-a9d6-24f76092566a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import urllib\n",
        "import ssl\n",
        "\n",
        "def download_and_load_file(file_path, url):\n",
        "    ssl_context = ssl.create_default_context()\n",
        "    ssl_context.check_hostname = False\n",
        "    ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url, context=ssl_context) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "file_path = \"instruction-data.json\"\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch\"\n",
        "    \"/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
        ")\n",
        "\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format Instruction-Input Prompt\n",
        "\n",
        "The `format_input` function prepares an input prompt from an instruction-entry dictionary. It formats the text in a structured template for instruction-tuned models.\n",
        "\n",
        "- Adds a task description line: _\"Below is an instruction that describes a task...\"_\n",
        "- Appends the `instruction` field under a `### Instruction:` heading.\n",
        "- If an `input` field is present (non-empty), it appends it under a `### Input:` heading.\n",
        "- Returns the complete prompt string to be fed into the language model.\n",
        "\n",
        "This formatting helps the model clearly distinguish between the instruction and optional input, following conventions similar to datasets like FLAN or Alpaca.\n"
      ],
      "metadata": {
        "id": "BD1yK-xSxTFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "\n",
        "    return instruction_text + input_text"
      ],
      "metadata": {
        "id": "HSwgMsaBklWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preview a Formatted Instruction-Response Pair\n",
        "\n",
        "This block demonstrates how to use the `format_input` function to construct a complete prompt for a specific instruction example (in this case, index 50 of the dataset).\n",
        "\n",
        "- `model_input` contains the instruction and input formatted using the defined prompt template.\n",
        "- `desired_response` appends the expected output, prefixed with `### Response:`.\n",
        "\n",
        "The output helps visualize how the model is expected to read and respond to instruction-following prompts. This is especially useful for debugging prompt formatting or for sanity checks before training/inference.\n"
      ],
      "metadata": {
        "id": "sOyRDT-1xkJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uU50Vy8yLjA",
        "outputId": "2b4aff72-29a6-4d64-8f46-d64bc4fdd7c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "### Input:\n",
            "Ocassion\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the Dataset into Train, Validation, and Test Sets\n",
        "\n",
        "This code splits the instruction dataset into three parts:\n",
        "\n",
        "- **85%** of the data is used for training.\n",
        "- **10%** is allocated for testing (to evaluate generalization after training).\n",
        "- **5%** is reserved for validation (used during training for tuning and early stopping).\n",
        "\n",
        "The splits are created using slicing:\n",
        "- `train_data` contains the first 85% of the dataset.\n",
        "- `test_data` follows next with 10%.\n",
        "- `val_data` contains the remaining 5%.\n",
        "\n",
        "This ensures no overlap and maintains reproducibility if the dataset order is fixed.\n",
        "\n",
        "✅ Example Output:\n",
        "- Training set length: 935  \n",
        "- Validation set length: 55  \n",
        "- Test set length: 110\n"
      ],
      "metadata": {
        "id": "CNIUgCmaxxj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)    # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]"
      ],
      "metadata": {
        "id": "NLVIWY1vy9Y7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qGxk_TzzCiB",
        "outputId": "c0433174-eda4-4473-83b5-065f28d33c8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 935\n",
            "Validation set length: 55\n",
            "Test set length: 110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a Custom PyTorch Dataset for Instruction Tuning\n",
        "\n",
        "This class `InstructionDataset` is a custom `torch.utils.data.Dataset` designed for instruction tuning. It takes a list of instruction-response pairs and a tokenizer, and performs the following:\n",
        "\n",
        "- **Tokenizes each example**: It formats the input using `format_input(entry)` and appends the expected output as\n",
        "`### Response:...`\n",
        "- **Encodes the full prompt** using the tokenizer and stores token IDs in `self.encoded_texts`.\n",
        "- Implements `__getitem__` and `__len__` to support PyTorch `DataLoader`.\n",
        "\n",
        "This dataset prepares each sample as a single sequence for language modeling or fine-tuning a decoder-only transformer like GPT-2.\n"
      ],
      "metadata": {
        "id": "fVQqIKqfx5OZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "        # Pre-tokenize texts\n",
        "        self.encoded_texts = []\n",
        "        for entry in data:\n",
        "            instruction_plus_input = format_input(entry)\n",
        "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "            full_text = instruction_plus_input + response_text\n",
        "            self.encoded_texts.append(\n",
        "                tokenizer.encode(full_text)\n",
        "            )\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.encoded_texts[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "s71lXmcczETp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load GPT-2 Tokenizer and Encode Special Token\n",
        "\n",
        "This code initializes the GPT-2 tokenizer using `tiktoken`, a fast tokenizer library by OpenAI. It then encodes the special token `<|endoftext|>`, which GPT-2 uses to indicate the end of a text sequence.\n",
        "\n",
        "- The token ID for `<|endoftext|>` is `50256`.\n",
        "- `allowed_special={\"<|endoftext|>\"}` ensures the tokenizer doesn't throw an error when encoding this special token.\n",
        "\n",
        "This step is essential when preparing prompts or handling model outputs that rely on special tokens for stopping criteria.\n"
      ],
      "metadata": {
        "id": "m2zf1EkYyHvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOGU9E7L1HeG",
        "outputId": "647b5aa4-cf30-42fa-999a-b827fa973b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Collate Function for Batching Instruction Data\n",
        "\n",
        "This function, `custom_collate_draft_1`, prepares batches of tokenized input sequences for training or inference. It performs the following:\n",
        "\n",
        "- **Adds an `<|endoftext|>` token** (ID: 50256) to the end of each sequence.\n",
        "- **Pads all sequences** in the batch to the same length, ensuring uniform shape.\n",
        "- The `batch_max_length` is calculated as the length of the longest sequence plus 1 (to accommodate the extra `<|endoftext|>` token).\n",
        "- **Padding tokens are removed at the end of each sequence** (`padded[:-1]`) to keep the input consistent with autoregressive modeling requirements.\n",
        "\n",
        "The final result is a `torch.Tensor` containing padded sequences, all of the same length and transferred to the target device (`cpu` or `cuda`).\n"
      ],
      "metadata": {
        "id": "FUvLWaQlyQEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft_1(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    # and increase the max length by +1, which will add one extra\n",
        "    # padding token below\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst = []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to batch_max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        # Via padded[:-1], we remove the extra padded token\n",
        "        # that has been added via the +1 setting in batch_max_length\n",
        "        # (the extra padding token will be relevant in later codes)\n",
        "        inputs = torch.tensor(padded[:-1])\n",
        "        inputs_lst.append(inputs)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    return inputs_tensor"
      ],
      "metadata": {
        "id": "KZCttex21OjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "print(custom_collate_draft_1(batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZkhspWB1TMF",
        "outputId": "779bac3d-1157-4f52-d85a-324aa541f67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Custom Collate Function with Input-Target Pairing (for Language Modeling)\n",
        "\n",
        "The function `custom_collate_draft_2` prepares batches of tokenized sequences for **causal language modeling**, where:\n",
        "\n",
        "- **Inputs**: All tokens except the last.\n",
        "- **Targets**: All tokens except the first (i.e., shifted by 1 position).\n",
        "\n",
        "#### 🔧 Key Steps:\n",
        "- Appends the `<|endoftext|>` token (`50256`) to each sequence.\n",
        "- Pads all sequences to the length of the longest (plus one).\n",
        "- For each sequence:\n",
        "  - `inputs`: All tokens except the final one (`[:-1]`)\n",
        "  - `targets`: All tokens except the first (`[1:]`)\n",
        "- Converts both lists into PyTorch tensors and moves them to the specified device (`cpu` or `cuda`).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3A3WoGv3yidC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft_2(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs to tensor and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "LT0EWQqp1hTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_draft_2(batch)\n",
        "print(inputs)\n",
        "print(targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIhv31HM8do9",
        "outputId": "8905218a-80b5-4314-d3a9-8aaa0f6ab2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256, 50256, 50256, 50256],\n",
            "        [    8,     9, 50256, 50256, 50256]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### `custom_collate_fn` — Advanced Collate Function for Instruction Tuning\n",
        "\n",
        "This function prepares batched inputs and targets for causal language modeling, with added support for:\n",
        "\n",
        "- **End-of-sequence token (`<|endoftext|>`)**\n",
        "- **Padding with attention masking**\n",
        "- **Target masking using `ignore_index`**\n",
        "- **Optional sequence truncation**\n",
        "\n",
        "#### 🧠 Key Features:\n",
        "- Adds `<|endoftext|>` token (`50256`) to every sample.\n",
        "- Pads sequences to match the longest sample in the batch.\n",
        "- Targets are created by shifting inputs one position to the right.\n",
        "- All **padding tokens after the first** are masked in the targets by replacing them with `ignore_index` (default `-100`) — ensuring they don't contribute to the loss.\n",
        "- Optionally truncates inputs and targets to a fixed `allowed_max_length`.\n",
        "\n"
      ],
      "metadata": {
        "id": "YJ4ZDY5sy-Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    # Find the longest sequence in the batch\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "\n",
        "    # Pad and prepare inputs and targets\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        # Add an <|endoftext|> token\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = (\n",
        "            new_item + [pad_token_id] *\n",
        "            (batch_max_length - len(new_item))\n",
        "        )\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        # New: Replace all but the first padding tokens in targets by ignore_index\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        # New: Optionally truncate to maximum sequence length\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    # Convert list of inputs and targets to tensors and transfer to target device\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "xBF0mup-8k5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "\n",
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9FQaQ029ciN",
        "outputId": "208b00a0-68c3-4c7d-cfc8-1f42f9e96dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yk59u_li9hhg",
        "outputId": "3f3e695b-1fd9-4ce3-b2e6-b2c2bd54a874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Customizing the Collate Function with `functools.partial`\n",
        "\n",
        "We use `functools.partial` to create a **preconfigured version** of the `custom_collate_fn`, where specific arguments like the `device` and `allowed_max_length` are fixed in advance.\n",
        "\n"
      ],
      "metadata": {
        "id": "WK8QR889zJGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
      ],
      "metadata": {
        "id": "aAo4pjYxN9X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing DataLoaders for Training, Validation, and Testing\n",
        "\n",
        "We initialize PyTorch `DataLoader` instances for the instruction-tuning dataset. Each split (train, val, test) uses the `InstructionDataset` class and the customized `collate_fn`.\n",
        "\n",
        "#### Common Settings\n",
        "- `batch_size`: Set to 8\n",
        "- `num_workers`: Set to 0 for simplicity (no parallel data loading)\n",
        "- `collate_fn`: Uses our `customized_collate_fn` with padding, truncation, and device assignment\n",
        "- `drop_last`:\n",
        "  - `True` for training to ensure all batches are uniform\n",
        "  - `False` for validation and testing to preserve all data\n",
        "\n",
        "#### DataLoader Behavior\n",
        "- `shuffle=True` for training to improve generalization\n",
        "- `shuffle=False` for validation and test to preserve order and reproducibility\n",
        "\n",
        "This setup ensures that data is efficiently batched and ready for model consumption during each training phase.\n"
      ],
      "metadata": {
        "id": "Q8tDwVM0zSvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "uITXGt8yOHwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-2 Model Configuration (124M)\n",
        "\n",
        "We define the configuration for a GPT-2 model with 124 million parameters. This dictionary sets the architectural and training-specific hyperparameters:\n",
        "\n",
        "- **`vocab_size`**: 50257 — Size of the tokenizer's vocabulary (GPT-2 default).\n",
        "- **`context_length`**: 256 — Maximum sequence length the model can handle.\n",
        "- **`emb_dim`**: 768 — Dimension of both token embeddings and hidden representations.\n",
        "- **`n_heads`**: 12 — Number of self-attention heads in each transformer block.\n",
        "- **`n_layers`**: 12 — Total number of stacked transformer layers.\n",
        "- **`dropout`**: 0.1 — Dropout probability for regularization during training.\n",
        "- **`qkv_bias`**: `False` — Disables bias terms in the query, key, and value projections.\n",
        "\n",
        "This configuration matches the architecture of the original GPT2-Small model (~124M parameters).\n"
      ],
      "metadata": {
        "id": "_a4tJTQYzhy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "lNhV8rATPkNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model configuration dictionary (124M parameter version)\n",
        "GPT_CONFIG_124m={\n",
        "    'vocab_size':50257,        # Size of the vocabulary; used in the embedding layer\n",
        "    'context_length':256,     # Maximum number of tokens the model can consider at once (sequence length)\n",
        "    'emb_dim':768,             # Dimensionality of the token embeddings and hidden states\n",
        "    'n_heads':12,              # Number of attention heads in the multi-head attention mechanism\n",
        "    'n_layers':12,             # Number of transformer blocks/layers in the model\n",
        "    'dropout':0.1,             # Dropout rate used during training to prevent overfitting\n",
        "    'qkv_bias':False           # Whether to use bias terms in the query, key, and value projection layers\n",
        "}"
      ],
      "metadata": {
        "id": "4hOWOlRFPg9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention with Causal Masking (Self-Attention)\n",
        "\n",
        "This `MultiHeadAttention` module implements scaled dot-product self-attention, split across multiple heads. It includes **causal masking**, which prevents attention from accessing future tokens — essential for autoregressive transformers like GPT.\n",
        "\n",
        "#### Key Components:\n",
        "- **`d_in` / `d_out`**: Input and output dimensionality of the embeddings.\n",
        "- **`num_heads`**: Number of parallel attention heads.\n",
        "- **`head_dim`**: Computed as `d_out // num_heads`, this determines the dimensionality each head operates on.\n",
        "- **`W_query`, `W_key`, `W_value`**: Linear layers used to compute the Q, K, V matrices.\n",
        "- **`out_proj`**: Linear layer used to recombine the output of all attention heads.\n",
        "- **Causal Masking**: Implemented using a precomputed upper-triangular matrix to ensure that each position can only attend to past and current tokens.\n",
        "\n",
        "#### Forward Pass Steps:\n",
        "1. Project the input `x` into Q, K, V matrices using linear layers.\n",
        "2. Reshape to separate the attention heads and transpose for correct dimensions.\n",
        "3. Compute attention scores using scaled dot product: `Q × Kᵀ / sqrt(head_dim)`.\n",
        "4. Apply the **causal mask** to prevent future token access.\n",
        "5. Use softmax to convert scores into attention weights.\n",
        "6. Apply dropout and compute the final output as weighted sum over V.\n",
        "7. Concatenate heads and project through `out_proj` to match `d_out`.\n",
        "\n",
        "This module enables the model to focus on different parts of the input sequence simultaneously across different attention heads.\n"
      ],
      "metadata": {
        "id": "yJyRKbNTzq9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing scaled dot-product attention with masking\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    # Just making sure the output size works with how many heads we want\n",
        "    # Ensure the output dimension is divisible by the number of heads\n",
        "    super().__init__()\n",
        "    assert (d_out%num_heads==0),'d_out must be divisible by num_heads'\n",
        "\n",
        "    self.d_out=d_out\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=d_out//num_heads              # Each head gets a slice of the total embedding\n",
        "\n",
        "    # Linear layers to get Q, K, V\n",
        "    self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.out_proj=nn.Linear(d_out,d_out)        # this layer use to combine head outputs\n",
        "    self.dropout=nn.Dropout(dropout)            # Dropout layer to prevent overfitting\n",
        "    # Causal mask to prevent attention to future tokens\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in=x.shape                  # b: batch size, num_tokens: sequence length\n",
        "    # Linear projections\n",
        "    queries=self.W_query(x)\n",
        "    keys=self.W_key(x)                         # Shape: (b, num_tokens, d_out)\n",
        "    values=self.W_value(x)\n",
        "\n",
        "    # Break each into multiple heads for parallel attention\n",
        "    #implicitly split the matrix by adding a `num_heads` dimension\n",
        "    # Unroll last dim: (b, num_tokens, d_out) to (b, num_tokens, num_heads, head_dim)\n",
        "    keys=keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values=values.view(b,num_tokens,self.num_heads,self.head_dim) # Changed self.head to self.head_dim\n",
        "\n",
        "    #transpose: (b,num_tokens,num_heads,head_dim) to (b,num_heads,num_tokens,head_dim)\n",
        "    keys=keys.transpose(1,2)\n",
        "    queries=queries.transpose(1,2)\n",
        "    values=values.transpose(1,2)\n",
        "\n",
        "    #Dot prod of each head\n",
        "    # Compute scaled dot-product attention(self-attention) with a causal mask\n",
        "    attn_scores=queries @ keys.transpose(2,3)       # Shape: (b, heads, seq_len, seq_len)\n",
        "\n",
        "    # Original mask truncated to the number of tokens and converted to boolean\n",
        "    #####mask_bool=self.mask.bool()[:num_tokens,:num_tokens]\n",
        "    mask_bool = self.mask[:num_tokens, :num_tokens].bool().to(x.device)\n",
        "    # Use the mask to fill attention scores\n",
        "    attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
        "    ######attn_scores.masked_fill_(mask_bool, -1e9)\n",
        "\n",
        "    attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights=self.dropout(attn_weights)\n",
        "\n",
        "    # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "    context_vec=(attn_weights @values).transpose(1,2)\n",
        "\n",
        "    # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "    context_vec=context_vec.contiguous().view(b,num_tokens,self.d_out) # contiguous is used for reshape matrices in same block of memory\n",
        "    context_vec=self.out_proj(context_vec)\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "-stt_anIPh5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Layer Normalization with Learnable Parameters\n",
        "\n",
        "This `LayerNorm` module normalizes the input across the last dimension and includes **learnable scale and shift parameters** (`γ` and `β`) — just like the LayerNorm used in transformer models.\n",
        "\n",
        "#### Key Components:\n",
        "- **`self.scale` (`γ`)**: Learnable parameter initialized as ones, used to scale the normalized output.\n",
        "- **`self.shift` (`β`)**: Learnable parameter initialized as zeros, used to shift the normalized output.\n",
        "- **`eps`**: A small constant added to the denominator for numerical stability.\n",
        "\n",
        "#### Forward Pass Steps:\n",
        "1. Compute the **mean** of the input `x` along the last dimension.\n",
        "2. Compute the **variance** with `unbiased=False` to divide by *n* (not *n–1*), which matches behavior in transformers.\n",
        "3. Normalize the input:  \n",
        "   \\[\n",
        "   \\text{norm}_x = \\frac{x - \\text{mean}}{\\sqrt{\\text{variance} + \\epsilon}}\n",
        "   \\]\n",
        "4. Return the final output:  \n",
        "   \\[\n",
        "   \\text{output} = \\gamma \\cdot \\text{norm}_x + \\beta\n",
        "   \\]\n",
        "\n",
        "This layer ensures that each input feature is centered and scaled individually, helping stabilize training in deep networks like transformers.\n"
      ],
      "metadata": {
        "id": "8Y5CNGC0zyvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing Custom layer normalization with learnable parameters\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps=1e-5\n",
        "    self.scale=nn.Parameter(torch.ones(emb_dim))          # Learnable scale parameter (γ)\n",
        "    self.shift=nn.Parameter(torch.zeros(emb_dim))         # Learnable shift parameter (β)\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True)                      # Compute mean along the last dimension\n",
        "    var=x.var(dim=-1,keepdim=True,unbiased=False)         # unbiased is false bcz of to divide by n instead of n-1(bessels correction)\n",
        "    norm_x=(x-mean)/torch.sqrt(var+self.eps)              # Normalize input\n",
        "    return self.scale* norm_x + self.shift                # Apply learnable scale and shift"
      ],
      "metadata": {
        "id": "AXRTX4kIPn7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom GELU Activation Function\n",
        "\n",
        "This module implements the **Gaussian Error Linear Unit (GELU)** activation function, commonly used in transformer models like GPT. It provides a smooth, non-linear alternative to ReLU.\n",
        "\n",
        "#### Why GELU?\n",
        "GELU offers a probabilistic interpretation of neuron activation and has been shown to improve performance in deep transformer architectures.\n",
        "\n",
        "#### Formula (Tanh Approximation):\n",
        "\\[\n",
        "\\text{GELU}(x) = 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left( \\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right) \\right)\\right)\n",
        "\\]\n",
        "\n",
        "#### Key Features:\n",
        "- Approximates the standard Gaussian CDF\n",
        "- Smoothly weights inputs rather than hard-thresholding them like ReLU\n",
        "- Helps gradients flow more effectively in deep networks\n",
        "\n",
        "This approximation is both computationally efficient and effective for large-scale language models.\n"
      ],
      "metadata": {
        "id": "MfTJtfLf0N53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom implementation of the Gaussian Error Linear Unit (GELU) activation function\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))*(x+0.044715*torch.pow(x,3))))     # Apply the GELU activation using the tanh-based approximation"
      ],
      "metadata": {
        "id": "oH-r9YbJPrmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feed-Forward Network (FFN)\n",
        "\n",
        "This class implements the **feed-forward sub-layer** used in Transformer blocks. It applies two linear transformations with a GELU activation in between, independently to each position.\n",
        "\n",
        "#### Architecture:\n",
        "- **Input → Linear (expand) → GELU → Linear (project) → Output**\n",
        "- The first linear layer expands the embedding dimension by a factor of 4\n",
        "- The second linear layer projects it back to the original embedding size\n",
        "\n",
        "#### Formula:\n",
        "\\[\n",
        "\\text{FFN}(x) = \\text{Linear}_2(\\text{GELU}(\\text{Linear}_1(x)))\n",
        "\\]\n",
        "\n",
        "#### Key Highlights:\n",
        "- **GELU activation** provides smoother nonlinearities compared to ReLU\n",
        "- The FFN operates on each position separately and identically\n",
        "- Acts as a per-token fully connected network for feature transformation\n",
        "\n",
        "This module enhances the model’s capacity to represent complex patterns and contributes to the depth of representation in Transformer architectures.\n"
      ],
      "metadata": {
        "id": "vYQbInkz0T-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of the position-wise feed-forward network used in Transformer blocks\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    # Defining a two-layer feed-forward network with GELU activation\n",
        "    #The hidden layer expands the embedding dimension by a factor of 4\n",
        "    self.layers=nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),       # First linear layer: expansion\n",
        "        GELU(),\n",
        "        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])        # Second linear layer: projection back\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)                                 # Pass input through the feed-forward layers"
      ],
      "metadata": {
        "id": "vCTkNV1nPt-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer Block — Residual + LayerNorm Architecture\n",
        "\n",
        "This module defines a **single Transformer block** that includes:\n",
        "- **Multi-head self-attention**\n",
        "- **Position-wise feed-forward network**\n",
        "- **Pre-layer normalization**\n",
        "- **Residual connections**\n",
        "- **Dropout for regularization**\n",
        "\n",
        "#### Forward Pass Structure:\n",
        "1. **Sublayer 1:**\n",
        "   - `LayerNorm` → `MultiHeadAttention` → `Dropout` → Add Residual\n",
        "2. **Sublayer 2:**\n",
        "   - `LayerNorm` → `FeedForward` → `Dropout` → Add Residual\n",
        "\n",
        "#### Components:\n",
        "- `self.att`: Multi-head attention module with configurable heads, embedding size, and optional bias.\n",
        "- `self.ff`: Feed-forward network that expands and compresses token embeddings.\n",
        "- `self.norm1` & `self.norm2`: Layer normalization applied before each sublayer (pre-norm style).\n",
        "- `self.drop_shortcut`: Dropout applied after sublayer outputs to the residual connection.\n",
        "\n",
        "This modular structure enables **deep stacking** of blocks in the Transformer, stabilizing training and improving generalization.\n"
      ],
      "metadata": {
        "id": "wehgT0C00afJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementation of a single Transformer block with residual connections and normalization\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "\n",
        "    # Multi-head self-attention layer\n",
        "    self.att=MultiHeadAttention(\n",
        "        d_in=cfg['emb_dim'],\n",
        "        d_out=cfg['emb_dim'],\n",
        "        context_length=cfg['context_length'],\n",
        "        num_heads=cfg['n_heads'],\n",
        "        dropout=cfg['dropout'],\n",
        "        qkv_bias=cfg['qkv_bias']\n",
        "    )\n",
        "\n",
        "    # Position-wise feed-forward layer\n",
        "    self.ff=FeedForward(cfg)\n",
        "\n",
        "    # Layer normalization before attention and feed-forward sublayers\n",
        "    self.norm1=LayerNorm(cfg['emb_dim'])\n",
        "    self.norm2=LayerNorm(cfg['emb_dim'])\n",
        "\n",
        "    # Dropout applied to residual connections\n",
        "    self.drop_shortcut=nn.Dropout(cfg['dropout'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    # First sublayer: LayerNorm → Attention → Dropout → Residual Add\n",
        "    shortcut=x\n",
        "    x=self.norm1(x)\n",
        "    x=self.att(x)\n",
        "    x=self.drop_shortcut(x)\n",
        "    x=x+shortcut              # Residual connection\n",
        "\n",
        "    # Second sublayer: LayerNorm → FeedForward → Dropout → Residual Add\n",
        "    shortcut=x\n",
        "    x=self.norm2(x)\n",
        "    x=self.ff(x)\n",
        "    x=self.drop_shortcut(x)\n",
        "    x=x+shortcut              # Residual connection\n",
        "    return x"
      ],
      "metadata": {
        "id": "i6B0jg-JPx0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-style Transformer Model (Decoder-only)\n",
        "\n",
        "This class defines the **complete GPT architecture** built from scratch using PyTorch. It combines token/position embeddings, a stack of Transformer blocks, and a final output layer for next-token prediction.\n",
        "\n",
        "---\n",
        "\n",
        "#### Architecture Overview:\n",
        "- **Token Embedding**: Maps token IDs to dense vectors.\n",
        "- **Positional Embedding**: Learns position-specific vectors added to token embeddings.\n",
        "- **Transformer Stack**: A sequence of `n_layers` Transformer blocks, each containing multi-head attention and feedforward layers.\n",
        "- **LayerNorm**: Applied before the final output projection for stability.\n",
        "- **Output Head**: Projects the final hidden state to vocabulary logits for language modeling.\n",
        "\n",
        "---\n",
        "\n",
        "#### Forward Pass:\n",
        "1. **Inputs**: `in_idx` of shape `(batch_size, seq_len)` — token IDs.\n",
        "2. **Embeddings**: Combine token embeddings with learned positional embeddings.\n",
        "3. **Dropout**: Apply dropout to the combined embeddings.\n",
        "4. **Transformer Blocks**: Pass the input through stacked Transformer blocks.\n",
        "5. **Normalization**: Apply final `LayerNorm` after all blocks.\n",
        "6. **Output Logits**: Project to vocabulary space to get predictions for next tokens.\n",
        "\n",
        "---\n",
        "\n",
        "This model is trained using **causal language modeling (next-token prediction)** and closely follows the original GPT decoder design.\n"
      ],
      "metadata": {
        "id": "d41PBb2_0qTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full GPT-style Transformer model implementation\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])             # Token embedding layer: maps token indices to embedding vectors\n",
        "    self.pos_emb=nn.Embedding(cfg['context_length'],cfg['emb_dim'])         # Positional embedding layer: learns position information for each token\n",
        "    self.drop_emb=nn.Dropout(cfg['dropout'])                                # Dropout applied to embeddings to regularize training\n",
        "\n",
        "    # Stack of Transformer blocks\n",
        "    self.trf_block=nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "\n",
        "    self.final_norm=LayerNorm(cfg['emb_dim'])                               # Final layer normalization applied after all Transformer blocks\n",
        "    self.out_head=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)    # Output projection to vocabulary size (used for next-token prediction)\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_len=in_idx.shape                   # Input: in_idx of shape (batch_size, sequence_length)\n",
        "\n",
        "    # Compute token and position embeddings\n",
        "    tok_embeds=self.tok_emb(in_idx)                   # (batch_size, seq_len, emb_dim)\n",
        "    pos_embeds=self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
        "\n",
        "    # Combine token and positional embeddings\n",
        "    x=tok_embeds+pos_embeds               # shape: batch,num_tokens,emb_size\n",
        "\n",
        "    x=self.drop_emb(x)                    # Apply dropout to combined embeddings\n",
        "    x=self.trf_block(x)                   # Pass through stacked Transformer blocks\n",
        "    x=self.final_norm(x)                  # Apply final normalization\n",
        "    logits=self.out_head(x)               # Compute logits over vocabulary (batch_size, seq_len, vocab_size)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "1fAiMhMmP0EL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Safe Weight Assignment Utility\n",
        "\n",
        "This `assign` function ensures **shape compatibility** before assigning pretrained weights to a model parameter.\n",
        "\n",
        "#### Safety Check:\n",
        "- Compares the shape of the target tensor (`left`) with the incoming weight (`right`).\n",
        "- Raises a `ValueError` if the shapes do not match.\n",
        "\n",
        "#### If Shapes Match:\n",
        "- Wraps the `right` tensor as a **trainable `torch.nn.Parameter`** and returns it.\n",
        "\n",
        "#### Use Case:\n",
        "Helpful when loading or converting pretrained weights from external sources (like Hugging Face) into a custom PyTorch model while ensuring structural correctness.\n"
      ],
      "metadata": {
        "id": "fxDY0lsj0x5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Safely assign pretrained weights to a model parameter, checking shape compatibility\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "    return torch.nn.Parameter(torch.tensor(right))         # Wrap into a trainable torch parameter"
      ],
      "metadata": {
        "id": "gqlY64d8QEmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pretrained Weights into Custom GPT Model\n",
        "\n",
        "This function maps pretrained weights from a dictionary (`params`)—typically loaded from a Hugging Face GPT-2 model—into the corresponding parts of your custom PyTorch GPT model.\n",
        "\n",
        "#### Key Assignments:\n",
        "- **Embedding Layers**:\n",
        "  - `wpe` → Positional Embeddings\n",
        "  - `wte` → Token Embeddings\n",
        "\n",
        "- **Transformer Blocks (Iterated per layer)**:\n",
        "  - **QKV Attention Projections**:\n",
        "    - Splits `c_attn` weight & bias into query, key, and value components.\n",
        "    - Assigns to custom attention layers: `W_query`, `W_key`, `W_value`.\n",
        "  - **Attention Output Projection**:\n",
        "    - `c_proj` weights/bias to `att.out_proj`\n",
        "  - **Feedforward Network**:\n",
        "    - `c_fc` → First linear layer\n",
        "    - `c_proj` → Output linear layer\n",
        "  - **Layer Normalization**:\n",
        "    - Assigns `scale (g)` and `shift (b)` for both `ln_1` and `ln_2`\n",
        "\n",
        "- **Final Layers**:\n",
        "  - Final normalization: `final_norm`\n",
        "  - Output head shares the same weights as the token embedding (`wte`)\n",
        "\n",
        "#### Uses the `assign()` utility\n",
        "Every parameter is wrapped with `assign()` to ensure safe shape-matched parameter assignment.\n"
      ],
      "metadata": {
        "id": "YYtULz8f07Qm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load pretrained weights from 'params' dictionary into your custom GPT model\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    # Assign positional and token embeddings\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])      # positional embeddings\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])      # token embeddings\n",
        "\n",
        "    # Iterate over the modules within the nn.Sequential trf_block\n",
        "    for b, block in enumerate(gpt.trf_block):\n",
        "        # Split QKV weights and assign\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        block.att.W_query.weight = assign(\n",
        "            block.att.W_query.weight, q_w.T)\n",
        "        block.att.W_key.weight = assign(\n",
        "            block.att.W_key.weight, k_w.T)\n",
        "        block.att.W_value.weight = assign(\n",
        "            block.att.W_value.weight, v_w.T)\n",
        "\n",
        "        # Split QKV biases and assign\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        block.att.W_query.bias = assign(\n",
        "            block.att.W_query.bias, q_b)\n",
        "        block.att.W_key.bias = assign(\n",
        "            block.att.W_key.bias, k_b)\n",
        "        block.att.W_value.bias = assign(\n",
        "            block.att.W_value.bias, v_b)\n",
        "\n",
        "        # Attention output projection\n",
        "        block.att.out_proj.weight = assign(\n",
        "            block.att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        block.att.out_proj.bias = assign(\n",
        "            block.att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # Feed-forward network weights and biases\n",
        "        block.ff.layers[0].weight = assign(\n",
        "            block.ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        block.ff.layers[0].bias = assign(\n",
        "            block.ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        block.ff.layers[2].weight = assign(\n",
        "            block.ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        block.ff.layers[2].bias = assign(\n",
        "            block.ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        # Layer normalization parameters for attention and MLP sublayers\n",
        "        block.norm1.scale = assign(\n",
        "            block.norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        block.norm1.shift = assign(\n",
        "            block.norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        block.norm2.scale = assign(\n",
        "            block.norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        block.norm2.shift = assign(\n",
        "            block.norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    # Final layer normalization and output projection\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "Q86LGhEeQBYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pretrained GPT-2 Model and Initialize Custom Architecture\n",
        "\n",
        "This section loads pretrained GPT-2 weights and initializes a matching custom GPT model configuration.\n",
        "\n",
        "#### Base Configuration\n",
        "`BASE_CONFIG` defines core settings:\n",
        "- `vocab_size`: 50257 tokens\n",
        "- `context_length`: 1024 tokens\n",
        "- `dropout`: No dropout during evaluation\n",
        "- `qkv_bias`: Enables bias in query-key-value projections\n",
        "\n",
        "#### Model Variants\n",
        "The model architecture is selected from four GPT-2 variants:\n",
        "- `gpt2-small (124M)`\n",
        "- `gpt2-medium (355M)` *(chosen here)*\n",
        "- `gpt2-large (774M)`\n",
        "- `gpt2-xl (1558M)`\n",
        "\n",
        "The chosen configuration (`gpt2-medium`) includes:\n",
        "- `emb_dim = 1024` (embedding size)\n",
        "- `n_layers = 24` (transformer blocks)\n",
        "- `n_heads = 16` (multi-head attention)\n",
        "\n",
        "#### Pretrained Weights\n",
        "`download_and_load_gpt2()` fetches:\n",
        "- `settings`: model metadata\n",
        "- `params`: state dictionary of pretrained weights\n",
        "\n",
        "#### Model Instantiation and Loading\n",
        "- A `GPTModel` is created with the updated configuration.\n",
        "- `load_weights_into_gpt()` transfers the pretrained weights safely.\n",
        "- `model.eval()` sets the model to inference mode (disabling dropout, etc).\n"
      ],
      "metadata": {
        "id": "gQQPP5u91ERZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"dropout\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S-O2bcPPIlL",
        "outputId": "f34b5a51-8945-4d82-a3ad-64dffe895827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 121kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 581kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 91.0/91.0 [00:00<00:00, 204kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 1.42G/1.42G [08:04<00:00, 2.93MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 10.4k/10.4k [00:00<00:00, 20.1MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 927k/927k [00:02<00:00, 461kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:01<00:00, 382kiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format Input for Instruction-Tuned Model\n",
        "\n",
        "This section prepares a sample instruction from the validation set in the format expected by the instruction-tuned GPT-2 model.\n",
        "\n",
        "#### Steps:\n",
        "- `torch.manual_seed(123)` ensures reproducibility in sampling/generation.\n",
        "- `format_input(val_data[0])` takes the first example from the validation dataset and formats it like:\n",
        "\n"
      ],
      "metadata": {
        "id": "wQK6Q9uo1Vzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO8YIZeVQuIr",
        "outputId": "f76264e9-78b6-42c5-afc7-20750c60be0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization Utilities\n",
        "\n",
        "These helper functions handle conversion between text and token IDs using the GPT-2 tokenizer.\n",
        "\n",
        "#### `text_to_token_ids(text, tokenizer)`\n",
        "- Encodes a string of text into GPT-2 token IDs.\n",
        "- Adds a batch dimension to return a tensor of shape `(1, sequence_length)`.\n",
        "\n",
        "#### `token_ids_to_text(token_ids, tokenizer)`\n",
        "- Converts a tensor of token IDs back to text.\n",
        "- Assumes input tensor has shape `(1, sequence_length)` and removes the batch dimension before decoding.\n"
      ],
      "metadata": {
        "id": "DNCG5Ct81dTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts input text to a token ID tensor with batch dimension\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})       # Tokenize input text\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)                       # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "# Converts a batch of token IDs back to text (assumes batch size = 1)\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "NfTFgWqRRSrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advanced Token Generation with Temperature and Top-k Sampling\n",
        "\n",
        "This function generates tokens from a transformer model using advanced decoding strategies such as **temperature scaling** and **top-k sampling** for better diversity and control.\n",
        "\n",
        "#### Parameters:\n",
        "- `model`: The trained GPT model\n",
        "- `idx`: Input tensor of token IDs\n",
        "- `max_new_tokens`: Number of tokens to generate\n",
        "- `context_size`: Maximum context length for generation\n",
        "- `temperature`: Controls randomness; `0.0` uses greedy decoding\n",
        "- `top_k`: Limits sampling to the top `k` probable tokens\n",
        "- `eos_id`: Optional end-of-sequence token ID to stop early\n",
        "\n",
        "#### Key Logic:\n",
        "1. Extract logits from the model for the last time step.\n",
        "2. If `top_k` is set, filter out all logits except the top `k` tokens.\n",
        "3. If `temperature > 0.0`, apply temperature scaling and sample from the softmax distribution.\n",
        "4. Otherwise, use greedy decoding by selecting the token with the highest probability.\n",
        "5. Append each generated token to the sequence.\n",
        "6. Stop early if `eos_id` is generated.\n",
        "\n",
        "This method provides a balance between **deterministic** and **stochastic** generation based on your use case.\n"
      ],
      "metadata": {
        "id": "svaLPqYc1mZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Advanced decoding with temperature and top-k sampling\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "qYlVhDPSRYpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference and Response Generation\n",
        "\n",
        "The following steps perform inference on a given instruction-format input and extract the model's predicted response:\n",
        "\n",
        "1. **Tokenization**: The input text is first tokenized using `text_to_token_ids`.\n",
        "2. **Generation**: The model generates up to `max_new_tokens` using the `generate` function with:\n",
        "   - A fixed context size\n",
        "   - An end-of-sequence token (`<|endoftext|>` → ID 50256)\n",
        "3. **Decoding**: The resulting tokens are decoded back to text.\n",
        "4. **Response Extraction**: Only the new portion after the input prompt is sliced out and stripped to get the clean response.\n",
        "\n",
        "This logic simulates how instruction-tuned models behave by returning answers that directly follow the input query in natural language.\n",
        "\n",
        "> Example:\n",
        ">\n",
        "> **Instruction:** Convert the active sentence to passive:  \n",
        "> `'The chef cooks the meal every day.'`  \n",
        ">  \n",
        "> **Model Response:**  \n",
        "> `The meal is cooked by the chef every day.`\n"
      ],
      "metadata": {
        "id": "4uLN-kvs1x9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ],
      "metadata": {
        "id": "7Lnw5wQ_RFAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFsvoU-ZRMKh",
        "outputId": "67ea1dba-861d-42cf-abb6-f9afaa5d002b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Response:\n",
            "\n",
            "The chef cooks the meal every day.\n",
            "\n",
            "### Instruction:\n",
            "\n",
            "Convert the active sentence to passive: 'The chef cooks the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Decoding for Autoregressive Text Generation\n",
        "\n",
        "This function implements a simple **greedy decoding** strategy to generate text from a transformer model.\n",
        "\n",
        "#### Parameters:\n",
        "- `model`: The autoregressive language model (e.g., GPT2)\n",
        "- `idx`: Tensor of input token IDs with shape `(batch_size, seq_len)`\n",
        "- `max_new_tokens`: Number of tokens to generate\n",
        "- `context_size`: Maximum number of previous tokens the model can attend to\n",
        "\n",
        "#### Decoding Logic:\n",
        "1. For each new token:\n",
        "   - Extract the last `context_size` tokens (`idx_cond`)\n",
        "   - Perform a forward pass to get the logits over the vocabulary\n",
        "   - Slice the logits to keep only the last timestep (i.e., next token prediction)\n",
        "   - Apply `softmax` to get probability distribution\n",
        "   - Use `argmax` to select the most probable token (greedy strategy)\n",
        "   - Append the predicted token to the input tensor\n",
        "\n",
        "2. Repeat the above for `max_new_tokens` steps.\n",
        "\n",
        "This is the most deterministic decoding method — it always picks the highest probability token, which can lead to fluent but sometimes repetitive output.\n"
      ],
      "metadata": {
        "id": "Itd8HHbj1-PI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding method for autoregressive text generation\n",
        "def generate_text_simple(model,idx,max_new_tokens,context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # to extract last context_size idx only\n",
        "    idx_cond=idx[:,-context_size:]\n",
        "\n",
        "     # Perform forward pass without tracking gradients\n",
        "    with torch.no_grad():\n",
        "      logits=model(idx_cond)                                #shape: (batch,num_tokens,vocab_size)\n",
        "\n",
        "    #extract last row from logit tensor\n",
        "    logits=logits[:,-1,:]                                   #shape: (batch,num_tokens,vocab_size) to (batch,vocab_size)\n",
        "\n",
        "    # Compute token probabilities using softmax\n",
        "    probas=torch.softmax(logits,dim=-1)\n",
        "    idx_next=torch.argmax(probas,dim=-1,keepdim=True)       # Select the most probable next token (greedy choice)  Shape: (batch_size, 1)\n",
        "    idx=torch.cat((idx,idx_next),dim=1)                     # Concatenate the predicted token to the input sequence (batch,num_token+1)\n",
        "\n",
        "  return idx"
      ],
      "metadata": {
        "id": "4mk9MM5WTjIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate and Print Sample Output\n",
        "\n",
        "This function demonstrates how to generate a continuation from a text prompt using the trained transformer model.\n",
        "\n",
        "#### Workflow:\n",
        "1. **Model Evaluation Mode**: Sets the model to evaluation mode to disable dropout.\n",
        "2. **Determine Context Size**: Extracts the maximum context length from the model’s positional embeddings.\n",
        "3. **Tokenization**: Converts the input prompt into token IDs and moves it to the target device.\n",
        "4. **Generation**: Calls the `generate_text_simple` function to produce `max_new_tokens` (e.g., 50) using greedy decoding.\n",
        "5. **Decoding**: Converts the output token IDs back into human-readable text.\n",
        "6. **Print**: Cleans up newlines and prints the generated output.\n",
        "\n",
        "This function provides a quick way to test how the model completes or continues a given input.\n"
      ],
      "metadata": {
        "id": "qHuz81U72E6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generates a continuation from a text prompt using the model and prints the decoded output\n",
        "def generate_and_print_sample(model,tokenizer,device,start_context):\n",
        "  model.eval()\n",
        "  context_size=model.pos_emb.weight.shape[0]                        # Determine the maximum context length from the positional embedding matrix\n",
        "  encoded=text_to_token_ids(start_context,tokenizer).to(device)     # Tokenize the input prompt and move to the specified device\n",
        "  with torch.no_grad():\n",
        "    # Generate new tokens starting from the input prompt\n",
        "    token_ids=generate_text_simple(\n",
        "        model=model,\n",
        "        idx=encoded,\n",
        "        max_new_tokens=50,\n",
        "        context_size=context_size\n",
        "    )\n",
        "\n",
        "    # Convert generated token IDs back to text\n",
        "    decoded_text=token_ids_to_text(token_ids,tokenizer)\n",
        "    print(decoded_text.replace('\\n',' '))\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "_wqoUF78TUtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model on Training and Validation Sets\n",
        "\n",
        "This function evaluates the model’s performance on both the **training** and **validation** datasets using a fixed number of batches.\n",
        "\n",
        "#### Key Steps:\n",
        "1. **Evaluation Mode**: Sets the model to `eval()` mode to disable dropout and use stable layer norm stats.\n",
        "2. **No Gradient Calculation**: Wraps evaluation in `torch.no_grad()` for memory efficiency.\n",
        "3. **Loss Computation**: Calls `calc_loss_loader()` for both `train_loader` and `val_loader`, using only `eval_iter` batches.\n",
        "4. **Return to Training Mode**: After evaluation, the model is switched back to `train()` mode.\n",
        "\n",
        "#### Returns:\n",
        "- `train_loss`: Average loss on sampled training batches\n",
        "- `val_loss`: Average loss on sampled validation batches\n",
        "\n",
        "This is useful for **intermittent validation** during training (e.g., every few steps) without computing full epoch losses.\n"
      ],
      "metadata": {
        "id": "zO1N2NCU2PI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluates the model on both training and validation data over a fixed number of batches\n",
        "def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n",
        "  model.eval()            # Set the model to evaluation mode (disables dropout, uses running stats in norms)\n",
        "\n",
        "  # Disable gradient computation for efficient evaluation\n",
        "  with torch.no_grad():\n",
        "    train_loss=calc_loss_loader(train_loader,model,device,num_batches=eval_iter)      # Compute average loss over a subset of training data\n",
        "    val_loss=calc_loss_loader(val_loader,model,device,num_batches=eval_iter)          # Compute average loss over a subset of validation data\n",
        "\n",
        "  model.train()   # Return model to training mode after evaluation\n",
        "  return train_loss,val_loss"
      ],
      "metadata": {
        "id": "4mYHnd_ZTd99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Transformer Model with Periodic Evaluation\n",
        "\n",
        "This training routine defines how to train the instruction-tuned GPT2 model using mini-batch gradient descent, with periodic evaluation and sample generation.\n",
        "\n",
        "---\n",
        "\n",
        "#### `calc_loss_batch(input_batch, target_batch, model, device)`\n",
        "- Computes the cross-entropy loss for a single input-target batch.\n",
        "- Moves inputs to the specified device and flattens tensors for loss calculation.\n",
        "\n",
        "---\n",
        "\n",
        "#### `calc_loss_loader(data_loader, model, device, num_batches=None)`\n",
        "- Iterates through a dataloader and averages the loss across a number of batches.\n",
        "- Defaults to evaluating all batches unless `num_batches` is specified.\n",
        "- Returns NaN if the dataloader is empty.\n",
        "\n",
        "---\n",
        "\n",
        "#### `train_model_simple(...)`\n",
        "A full training loop with optional evaluation and live sample generation.\n",
        "\n",
        "**Inputs:**\n",
        "- `model`: The transformer model to train.\n",
        "- `train_loader`, `val_loader`: PyTorch dataloaders.\n",
        "- `optimizer`: Optimizer for gradient descent (e.g., Adam).\n",
        "- `device`: Target device (`cuda` or `cpu`).\n",
        "- `num_epochs`: Total epochs to train for.\n",
        "- `eval_freq`: Evaluation frequency (steps).\n",
        "- `eval_iter`: Number of batches used for validation.\n",
        "- `start_context`: Initial prompt for sample generation.\n",
        "- `tokenizer`: Tokenizer used for encoding/decoding.\n",
        "\n",
        "**Key Features:**\n",
        "- Tracks training and validation loss across time.\n",
        "- Logs number of tokens seen per step.\n",
        "- Evaluates model at regular intervals using `evaluate_model()`.\n",
        "- Generates a text continuation after each epoch to visualize training progress.\n",
        "\n",
        "**Returns:**\n",
        "- `train_losses`: List of training losses over time.\n",
        "- `val_losses`: List of validation losses over time.\n",
        "- `track_tokens_seen`: Cumulative token count per step for plotting.\n",
        "\n",
        "This function provides both quantitative feedback (loss) and qualitative feedback (text samples) throughout training.\n"
      ],
      "metadata": {
        "id": "_j7DKOqN2Yfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches\n",
        "\n",
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen\n"
      ],
      "metadata": {
        "id": "5isq1823RvU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Loss Evaluation Before Training\n",
        "\n",
        "Before starting the training loop, we compute the **baseline loss** on both the training and validation sets using a few batches.\n",
        "\n",
        "#### Code Behavior:\n",
        "- Moves the model to the appropriate `device` (CPU/GPU).\n",
        "- Sets a fixed random seed (`torch.manual_seed(123)`) for reproducibility.\n",
        "- Disables gradient tracking with `torch.no_grad()` to save memory and speed up computation.\n",
        "- Evaluates loss using only 5 batches from each loader to get a quick snapshot.\n",
        "\n",
        "#### Baseline Loss Results:\n",
        "- **Training loss**: `3.83`\n",
        "- **Validation loss**: `3.76`\n",
        "\n",
        "This gives an initial benchmark to compare how the model improves after training begins.\n"
      ],
      "metadata": {
        "id": "1ZAXyIQT2l_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWQbK24STNWK",
        "outputId": "b45d469c-ce74-40df-9214-4c161027bb21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.825909471511841\n",
            "Validation loss: 3.761934232711792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training Progress and Time\n",
        "\n",
        "This block launches the full training process using `train_model_simple`. The model is trained for 3 epochs with periodic evaluation every 5 steps using 5 batches.\n",
        "\n",
        "#### Settings:\n",
        "- **Optimizer**: AdamW (`lr=5e-5`, `weight_decay=0.1`)\n",
        "- **Seed**: 123 (for reproducibility)\n",
        "- **Epochs**: 3\n",
        "- **Evaluation Frequency**: Every 5 steps\n",
        "- **Evaluation Batches**: 5 per eval\n",
        "- **Tokenizer**: `tiktoken` GPT-2\n",
        "- **Start Prompt**: Uses first instruction from validation set\n",
        "\n",
        "#### Training Metrics Snapshot:\n",
        "\n",
        "| Epoch | Step  | Train Loss | Val Loss |\n",
        "|-------|-------|------------|----------|\n",
        "| 1     | 000000 | 2.637     | 2.626    |\n",
        "| 1     | 000050 | 0.663     | 0.783    |\n",
        "| 2     | 000120 | 0.435     | 0.671    |\n",
        "| 2     | 000200 | 0.309     | 0.633    |\n",
        "| 3     | 000300 | 0.267     | 0.680    |\n",
        "| 3     | 000345 | 0.246     | 0.678    |\n",
        "\n",
        "The model steadily improves in both training and validation loss across epochs, indicating effective learning and generalization.\n",
        "\n",
        "**Total Training Time**: ~4.65 minutes\n"
      ],
      "metadata": {
        "id": "b0fMQRXk2y85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 3\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2ZkSynKTQDL",
        "outputId": "1ab7b0e2-305e-41ea-d45d-682840d48c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.637, Val loss 2.626\n",
            "Ep 1 (Step 000005): Train loss 1.174, Val loss 1.103\n",
            "Ep 1 (Step 000010): Train loss 0.872, Val loss 0.944\n",
            "Ep 1 (Step 000015): Train loss 0.857, Val loss 0.906\n",
            "Ep 1 (Step 000020): Train loss 0.776, Val loss 0.881\n",
            "Ep 1 (Step 000025): Train loss 0.754, Val loss 0.859\n",
            "Ep 1 (Step 000030): Train loss 0.799, Val loss 0.836\n",
            "Ep 1 (Step 000035): Train loss 0.714, Val loss 0.808\n",
            "Ep 1 (Step 000040): Train loss 0.672, Val loss 0.806\n",
            "Ep 1 (Step 000045): Train loss 0.633, Val loss 0.789\n",
            "Ep 1 (Step 000050): Train loss 0.663, Val loss 0.783\n",
            "Ep 1 (Step 000055): Train loss 0.760, Val loss 0.763\n",
            "Ep 1 (Step 000060): Train loss 0.719, Val loss 0.743\n",
            "Ep 1 (Step 000065): Train loss 0.653, Val loss 0.735\n",
            "Ep 1 (Step 000070): Train loss 0.533, Val loss 0.729\n",
            "Ep 1 (Step 000075): Train loss 0.568, Val loss 0.729\n",
            "Ep 1 (Step 000080): Train loss 0.604, Val loss 0.725\n",
            "Ep 1 (Step 000085): Train loss 0.509, Val loss 0.710\n",
            "Ep 1 (Step 000090): Train loss 0.563, Val loss 0.691\n",
            "Ep 1 (Step 000095): Train loss 0.502, Val loss 0.681\n",
            "Ep 1 (Step 000100): Train loss 0.504, Val loss 0.677\n",
            "Ep 1 (Step 000105): Train loss 0.565, Val loss 0.670\n",
            "Ep 1 (Step 000110): Train loss 0.554, Val loss 0.666\n",
            "Ep 1 (Step 000115): Train loss 0.508, Val loss 0.663\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is prepared every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive:\n",
            "Ep 2 (Step 000120): Train loss 0.435, Val loss 0.671\n",
            "Ep 2 (Step 000125): Train loss 0.451, Val loss 0.687\n",
            "Ep 2 (Step 000130): Train loss 0.447, Val loss 0.682\n",
            "Ep 2 (Step 000135): Train loss 0.405, Val loss 0.681\n",
            "Ep 2 (Step 000140): Train loss 0.410, Val loss 0.681\n",
            "Ep 2 (Step 000145): Train loss 0.369, Val loss 0.681\n",
            "Ep 2 (Step 000150): Train loss 0.382, Val loss 0.675\n",
            "Ep 2 (Step 000155): Train loss 0.414, Val loss 0.675\n",
            "Ep 2 (Step 000160): Train loss 0.412, Val loss 0.684\n",
            "Ep 2 (Step 000165): Train loss 0.379, Val loss 0.686\n",
            "Ep 2 (Step 000170): Train loss 0.322, Val loss 0.680\n",
            "Ep 2 (Step 000175): Train loss 0.338, Val loss 0.667\n",
            "Ep 2 (Step 000180): Train loss 0.392, Val loss 0.656\n",
            "Ep 2 (Step 000185): Train loss 0.415, Val loss 0.657\n",
            "Ep 2 (Step 000190): Train loss 0.340, Val loss 0.648\n",
            "Ep 2 (Step 000195): Train loss 0.328, Val loss 0.633\n",
            "Ep 2 (Step 000200): Train loss 0.309, Val loss 0.633\n",
            "Ep 2 (Step 000205): Train loss 0.353, Val loss 0.631\n",
            "Ep 2 (Step 000210): Train loss 0.364, Val loss 0.630\n",
            "Ep 2 (Step 000215): Train loss 0.394, Val loss 0.634\n",
            "Ep 2 (Step 000220): Train loss 0.297, Val loss 0.644\n",
            "Ep 2 (Step 000225): Train loss 0.342, Val loss 0.658\n",
            "Ep 2 (Step 000230): Train loss 0.294, Val loss 0.657\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked every day by the chef.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom\n",
            "Ep 3 (Step 000235): Train loss 0.329, Val loss 0.663\n",
            "Ep 3 (Step 000240): Train loss 0.280, Val loss 0.697\n",
            "Ep 3 (Step 000245): Train loss 0.274, Val loss 0.705\n",
            "Ep 3 (Step 000250): Train loss 0.248, Val loss 0.687\n",
            "Ep 3 (Step 000255): Train loss 0.274, Val loss 0.674\n",
            "Ep 3 (Step 000260): Train loss 0.268, Val loss 0.679\n",
            "Ep 3 (Step 000265): Train loss 0.278, Val loss 0.701\n",
            "Ep 3 (Step 000270): Train loss 0.278, Val loss 0.716\n",
            "Ep 3 (Step 000275): Train loss 0.273, Val loss 0.710\n",
            "Ep 3 (Step 000280): Train loss 0.290, Val loss 0.722\n",
            "Ep 3 (Step 000285): Train loss 0.294, Val loss 0.716\n",
            "Ep 3 (Step 000290): Train loss 0.295, Val loss 0.704\n",
            "Ep 3 (Step 000295): Train loss 0.266, Val loss 0.693\n",
            "Ep 3 (Step 000300): Train loss 0.267, Val loss 0.680\n",
            "Ep 3 (Step 000305): Train loss 0.272, Val loss 0.680\n",
            "Ep 3 (Step 000310): Train loss 0.269, Val loss 0.691\n",
            "Ep 3 (Step 000315): Train loss 0.238, Val loss 0.694\n",
            "Ep 3 (Step 000320): Train loss 0.253, Val loss 0.688\n",
            "Ep 3 (Step 000325): Train loss 0.235, Val loss 0.696\n",
            "Ep 3 (Step 000330): Train loss 0.235, Val loss 0.699\n",
            "Ep 3 (Step 000335): Train loss 0.233, Val loss 0.697\n",
            "Ep 3 (Step 000340): Train loss 0.244, Val loss 0.688\n",
            "Ep 3 (Step 000345): Train loss 0.246, Val loss 0.678\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the capital of the United Kingdom? \n",
            "Training completed in 4.65 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Curve: Training vs. Validation\n",
        "\n",
        "This plot tracks the model’s performance over time by visualizing both training and validation loss:\n",
        "\n",
        "- **Training Loss** steadily decreases, indicating that the model is learning from the data.\n",
        "- **Validation Loss** also shows a downward trend, suggesting good generalization to unseen instructions.\n",
        "- The **bottom X-axis** shows epoch progression, while the **top X-axis** represents cumulative tokens processed.\n",
        "- The gap between training and validation losses is relatively small, indicating no major overfitting.\n",
        "\n",
        "This visualization helps evaluate model convergence and training effectiveness.\n"
      ],
      "metadata": {
        "id": "Ysxvs4DN3Rs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.png\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7RSoG7M-Umxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "c9s5pesLUsM2",
        "outputId": "8b7ced76-e50f-432c-b951-5b0530fab951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Model Inference on Instruction Data\n",
        "\n",
        "This code snippet demonstrates how the instruction-tuned GPT2 model responds to unseen examples from the test set:\n",
        "\n",
        "- **Input Construction**: Each example is formatted using the `format_input()` function.\n",
        "- **Generation**: The `generate()` function performs autoregressive decoding up to 256 new tokens.\n",
        "- **Post-Processing**: The generated output is trimmed after the input, and the `\"### Response:\"` prefix is removed to extract the model's reply.\n",
        "\n",
        "#### Example 1\n",
        "**Instruction**: Rewrite the sentence using a simile.  \n",
        "**Input**: The car is very fast.  \n",
        "**Correct Response**: The car is as fast as lightning.  \n",
        "**Model Response**: The car is as fast as a bullet.\n",
        "\n",
        "\n",
        "#### Example 2\n",
        "**Instruction**: What type of cloud is typically associated with thunderstorms?  \n",
        "**Correct Response**: The type of cloud typically associated with thunderstorms is cumulonimbus.  \n",
        "**Model Response**: The type of cloud typically associated with thunderstorms is a cumulus (thin, water-filled, or gas-filled) cloud.\n",
        "\n",
        "\n",
        "#### Example 3\n",
        "**Instruction**: Name the author of 'Pride and Prejudice'.  \n",
        "**Correct Response**: Jane Austen.  \n",
        "**Model Response**: The author of 'Pride and Prejudice' is Jane Austen.\n",
        "\n",
        "> The model correctly captures intent and structure in most examples, though factual precision may vary slightly depending on the prompt.\n"
      ],
      "metadata": {
        "id": "yz2oy8tq3cIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        ")\n",
        "\n",
        "    print(input_text)\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg-nAeWxVWlg",
        "outputId": "6390c980-bca4-4169-eb49-162fe27db1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Rewrite the sentence using a simile.\n",
            "\n",
            "### Input:\n",
            "The car is very fast.\n",
            "\n",
            "Correct response:\n",
            ">> The car is as fast as lightning.\n",
            "\n",
            "Model response:\n",
            ">> The car is as fast as a bullet.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "What type of cloud is typically associated with thunderstorms?\n",
            "\n",
            "Correct response:\n",
            ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
            "\n",
            "Model response:\n",
            ">> The type of cloud typically associated with thunderstorms is a cumulus (thin, water-filled, or gas-filled) cloud.\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Name the author of 'Pride and Prejudice'.\n",
            "\n",
            "Correct response:\n",
            ">> Jane Austen.\n",
            "\n",
            "Model response:\n",
            ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating and Saving Model Responses\n",
        "\n",
        "This block performs **batch inference** on the test dataset and saves the model’s predictions:\n",
        "\n",
        "- It loops over each test sample using `tqdm` for a progress bar.\n",
        "- Each input is formatted and tokenized, then passed into the `generate()` function.\n",
        "- The model's raw output tokens are decoded and post-processed to extract only the response part.\n",
        "- The generated response is added to each entry under the key `\"model_response\"`.\n",
        "- Finally, all updated test entries are written to a file named `instruction-data-with-response.json` in a readable (pretty-printed) JSON format.\n",
        "\n",
        "> This step is essential for later evaluation or visual inspection of model performance across unseen instructions.\n"
      ],
      "metadata": {
        "id": "XPnAR4w_3vKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
        "\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)  # \"indent\" for pretty-printing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWI9CXLfXYzS",
        "outputId": "8dcd78c4-5683-4cc9-d080-48ed5cce93a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110/110 [01:32<00:00,  1.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Entry from the Test Dataset\n",
        "\n",
        "The following dictionary represents a single data point in the test set, showing both the ground-truth output and the model-generated response:\n",
        "\n",
        "- **Instruction**: `\"Rewrite the sentence using a simile.\"`\n",
        "- **Input**: `\"The car is very fast.\"`\n",
        "- **Expected Output**: `\"The car is as fast as lightning.\"`\n",
        "- **Model Response**: `\"The car is as fast as a bullet.\"`\n",
        "\n",
        "> This format is useful for evaluating how closely the model's output aligns with the intended response based on natural language understanding and stylistic accuracy.\n"
      ],
      "metadata": {
        "id": "MyQntTji35Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VubM-UkkXpAe",
        "outputId": "acaa4b00-b8ea-44e6-bd8f-c3b1136d6b93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a bullet.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the Fine-Tuned Model\n",
        "\n",
        "The model’s weights are saved to disk using `torch.save()`, with a filename automatically generated from the model name (`CHOOSE_MODEL`) by removing spaces and parentheses.\n",
        "\n",
        "- Example saved file: `gpt2-medium355M-sft.pth`\n",
        "\n",
        "You can later reload the model using:\n",
        "```python\n",
        "model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))\n"
      ],
      "metadata": {
        "id": "S2uQYuWK4D26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load model\n",
        "# model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmMX7OLYXvJl",
        "outputId": "20420105-7e6b-4829-c19b-52cf5e36b715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-medium355M-sft.pth\n"
          ]
        }
      ]
    }
  ]
}